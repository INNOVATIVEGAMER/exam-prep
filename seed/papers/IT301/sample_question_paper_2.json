{
  "subject_code": "IT301",
  "title": "Sample Question Paper 2",
  "type": "end_sem",
  "year": "2025-26",
  "is_free": false,
  "price": 4900,
  "metadata": {},
  "questions": {
    "A1": {
      "group": "A",
      "number": "A1",
      "text": "The register that holds the address of the next instruction to be fetched is:",
      "marks": 1,
      "co": "CO1",
      "bl": "L1",
      "options": [
        {
          "key": "a",
          "text": "Accumulator (AC)"
        },
        {
          "key": "b",
          "text": "Program Counter (PC)"
        },
        {
          "key": "c",
          "text": "Memory Address Register (MAR)"
        },
        {
          "key": "d",
          "text": "Instruction Register (IR)"
        }
      ]
    },
    "A2": {
      "group": "A",
      "number": "A2",
      "text": "In a 4-bit ripple carry adder, the worst-case carry propagation passes through:",
      "marks": 1,
      "co": "CO2",
      "bl": "L2",
      "options": [
        {
          "key": "a",
          "text": "1 full adder"
        },
        {
          "key": "b",
          "text": "2 full adders"
        },
        {
          "key": "c",
          "text": "3 full adders"
        },
        {
          "key": "d",
          "text": "4 full adders"
        }
      ]
    },
    "A3": {
      "group": "A",
      "number": "A3",
      "text": "Which cache mapping technique offers the best hit ratio but is most expensive?",
      "marks": 1,
      "co": "CO2",
      "bl": "L2",
      "options": [
        {
          "key": "a",
          "text": "Direct mapped"
        },
        {
          "key": "b",
          "text": "Fully associative"
        },
        {
          "key": "c",
          "text": "Set associative"
        },
        {
          "key": "d",
          "text": "None of these"
        }
      ]
    },
    "A4": {
      "group": "A",
      "number": "A4",
      "text": "The bias value used in IEEE 754 single-precision exponent is:",
      "marks": 1,
      "co": "CO1",
      "bl": "L1",
      "options": [
        {
          "key": "a",
          "text": "64"
        }
      ]
    },
    "A5": {
      "group": "A",
      "number": "A5",
      "text": "Which I/O technique allows data transfer between I/O device and memory without CPU inter- vention?",
      "marks": 1,
      "co": "CO2",
      "bl": "L1",
      "options": [
        {
          "key": "a",
          "text": "Programmed I/O"
        },
        {
          "key": "b",
          "text": "Interrupt-driven I/O"
        },
        {
          "key": "c",
          "text": "DMA"
        },
        {
          "key": "d",
          "text": "Polling"
        }
      ]
    },
    "A6": {
      "group": "A",
      "number": "A6",
      "text": "In a Write-Back cache policy, data is written to main memory:",
      "marks": 1,
      "co": "CO2",
      "bl": "L2",
      "options": [
        {
          "key": "a",
          "text": "Every time cache is updated"
        },
        {
          "key": "b",
          "text": "Only when the cache line is replaced"
        },
        {
          "key": "c",
          "text": "At fixed time intervals"
        },
        {
          "key": "d",
          "text": "Never"
        }
      ]
    },
    "A7": {
      "group": "A",
      "number": "A7",
      "text": "What type of hazard occurs when two instructions try to access the same hardware resource simultaneously?",
      "marks": 1,
      "co": "CO2",
      "bl": "L1",
      "options": [
        {
          "key": "a",
          "text": "Data hazard"
        },
        {
          "key": "b",
          "text": "Control hazard"
        },
        {
          "key": "c",
          "text": "Structural hazard"
        },
        {
          "key": "d",
          "text": "RAW hazard"
        }
      ]
    },
    "A8": {
      "group": "A",
      "number": "A8",
      "text": "The process of dividing a program into fixed-size blocks for virtual memory management is called:",
      "marks": 1,
      "co": "CO2",
      "bl": "L1",
      "options": [
        {
          "key": "a",
          "text": "Segmentation"
        },
        {
          "key": "b",
          "text": "Paging"
        },
        {
          "key": "c",
          "text": "Fragmentation"
        },
        {
          "key": "d",
          "text": "Compaction"
        }
      ]
    },
    "A9": {
      "group": "A",
      "number": "A9",
      "text": "Which of the following is true about CISC architecture?",
      "marks": 1,
      "co": "CO1",
      "bl": "L1",
      "options": [
        {
          "key": "a",
          "text": "Uses hardwired control unit only"
        },
        {
          "key": "b",
          "text": "Has a small set of simple instructions"
        },
        {
          "key": "c",
          "text": "Instructions may take multiple clock cycles"
        },
        {
          "key": "d",
          "text": "Does not support microprogramming"
        }
      ]
    },
    "A10": {
      "group": "A",
      "number": "A10",
      "text": "The inclusion property in memory hierarchy states that:",
      "marks": 1,
      "co": "CO2",
      "bl": "L2",
      "options": [
        {
          "key": "a",
          "text": "Lower level contains a subset of upper level"
        },
        {
          "key": "b",
          "text": "Upper level contains a subset of lower level"
        },
        {
          "key": "c",
          "text": "All levels contain the same data"
        },
        {
          "key": "d",
          "text": "Levels are independent"
        }
      ]
    },
    "A11": {
      "group": "A",
      "number": "A11",
      "text": "Systolic arrays are best suited for:",
      "marks": 1,
      "co": "CO3",
      "bl": "L1",
      "options": [
        {
          "key": "a",
          "text": "General purpose computing"
        },
        {
          "key": "b",
          "text": "Matrix operations and signal processing"
        },
        {
          "key": "c",
          "text": "Operating system scheduling"
        }
      ]
    },
    "A12": {
      "group": "A",
      "number": "A12",
      "text": "In a daisy-chain priority interrupt system, the device with the highest priority is:",
      "marks": 1,
      "co": "CO2",
      "bl": "L1",
      "options": [
        {
          "key": "a",
          "text": "The one farthest from the CPU"
        },
        {
          "key": "b",
          "text": "The one closest to the CPU"
        },
        {
          "key": "c",
          "text": "Randomly assigned"
        },
        {
          "key": "d",
          "text": "Determined by software"
        }
      ]
    },
    "B1": {
      "group": "B",
      "number": "B1",
      "text": "Explain the role of an operating system and compiler/assembler in the execution of a program on a stored program computer.",
      "marks": 5,
      "co": "CO1",
      "bl": "L2"
    },
    "B2": {
      "group": "B",
      "number": "B2",
      "text": "Compare RISC and CISC architectures on the basis of instruction set, control unit design, clock cycles per instruction, and addressing modes.",
      "marks": 5,
      "co": "CO1",
      "bl": "L4"
    },
    "B3": {
      "group": "B",
      "number": "B3",
      "text": "(a) Differentiate between static RAM (SRAM) and dynamic RAM (DRAM).[3] (b) Why does DRAM require periodic refreshing? Explain briefly.[2]",
      "marks": 5,
      "co": "CO2",
      "bl": "L2"
    },
    "B4": {
      "group": "B",
      "number": "B4",
      "text": "Explain the three types of pipeline hazards with one example each. How does operand forward- ing help resolve data hazards?",
      "marks": 5,
      "co": "CO2",
      "bl": "L4"
    },
    "B5": {
      "group": "B",
      "number": "B5",
      "text": "(a) What is Instruction-Level Parallelism (ILP)?[2] (b) Briefly compare superscalar and super-pipelined processor architectures.[3]",
      "marks": 5,
      "co": "CO3",
      "bl": "L1/"
    },
    "C1": {
      "group": "C",
      "number": "C1",
      "text": "(a) Describe the design of an ALU. Explain how a 4-bit arithmetic unit performs addition and subtraction using full adders and control logic.[6] (b) Apply Booth’s multiplication algorithm to multiply (+9)×(−6). Use 5-bit signed represen- tation.[5] (c) Explain overflow and underflow conditions in fixed-point arithmetic. How are they detected in 2’s complement representation?[4]",
      "marks": 15,
      "co": "CO1",
      "bl": "L2"
    },
    "C2": {
      "group": "C",
      "number": "C2",
      "text": "(a) Explain memory hierarchy with a diagram. Discuss the trade-offs between speed, cost, and capacity.[5] (b) A computer has 4 MB main memory and 256 KB cache. Block size = 64 bytes. Deter- mine tag, index, and offset fields for: (i) Direct Mapped, (ii) Fully Associative, (iii) 8-way Set Associative.[6] (c) Explain the coherence and locality properties of memory. How does spatial and temporal locality help in cache design?[4]",
      "marks": 15,
      "co": "CO2",
      "bl": "L2"
    },
    "C3": {
      "group": "C",
      "number": "C3",
      "text": "(a) Design a hardwired control unit for a basic computer. Explain the role of the instruction decoder, sequence counter, and control logic gates.[6] (b) Compare hardwired and microprogrammed control units. Discuss horizontal and vertical microinstructions.[5] (c) Explain polled I/O and interrupt-driven I/O. Why is interrupt-driven I/O preferred?[4]",
      "marks": 15,
      "co": "CO2",
      "bl": "L2"
    },
    "C4": {
      "group": "C",
      "number": "C4",
      "text": "(a) Explain instruction pipeline and arithmetic pipeline. Draw a space-time diagram for a 4-stage instruction pipeline processing 6 instructions.[5] (b) A five-stage pipeline has stage delays of 120, 100, 150, 130, and 110 ns. Inter-stage registers have a delay of 5 ns each. (i) Clock cycle time? (ii) Time to process 500 tasks? (iii) Speedup over non-pipelined?[6] (c) Discuss pipeline optimization techniques: delayed branching, branch prediction, and instruc- tion reordering.[4]",
      "marks": 15,
      "co": "CO2",
      "bl": "L2"
    }
  },
  "answers": {
    "A1": {
      "question_number": "A1",
      "correct_option": "b",
      "solution": "TheProgram Counter (PC)always holds the address of thenext instructionto be fetched.\nAfter each fetch, PC is automatically incremented. During branch instructions, PC is loaded\nwith the target address. MAR holds the current memory address being accessed; IR holds the\nfetched instruction; AC holds computation results."
    },
    "A2": {
      "question_number": "A2",
      "correct_option": "d",
      "solution": "In a ripple carry adder, the carry must propagate throughallfull adders sequentially from LSB\nto MSB. For a 4-bit RCA, the worst case (e.g., adding 0111 + 0001) requires the carry to ripple\nthrough all4 full adders. This gives a delay of 4×2 = 8 gate delays (each full adder has 2\ngate delays for carry)."
    },
    "A3": {
      "question_number": "A3",
      "correct_option": "b",
      "solution": "Fully associativemapping allows any memory block to be placed inanycache line, eliminating\nconflict misses entirely. This gives the best hit ratio. However, it requires comparing the\ntag of every cache line simultaneously (using expensive CAM hardware). Cost scales asO(n)\ncomparators fornlines."
    },
    "A4": {
      "question_number": "A4",
      "solution": "— 2 —\nJISCE / IT / R23 / IT301 / Sample Paper 2COA — Answer Key\n(b) 127\n(c) 128\n(d) 1023\nAnswer: (b)\nBias = 2\nk−1\n−1 wherek= exponent bits. For single precision (k= 8): bias = 2\n7\n−1 =127.\nFor double precision (k= 11): bias = 1023. The stored exponent = actual exponent + bias."
    },
    "A5": {
      "question_number": "A5",
      "correct_option": "c",
      "solution": "DMA (Direct Memory Access)enables the DMA controller to transfer data directly be-\ntween I/O devices and memory. The CPU only initiates the transfer (provides address, count,\ndirection) and is interrupted upon completion. Programmed I/O and polling both require CPU\ninvolvement for every word transferred."
    },
    "A6": {
      "question_number": "A6",
      "correct_option": "b",
      "solution": "InWrite-Backpolicy, writes are made only to the cache. The modified data is written to main\nmemoryonly when the cache line is evicted/replaced. A dirty bit tracks modifications.\nThis reduces memory traffic compared to Write-Through (option a), which writes to both cache\nand memory on every write."
    },
    "A7": {
      "question_number": "A7",
      "correct_option": "c",
      "solution": "Structural hazardsoccur when the hardware cannot support all possible instruction combi-\nnations in the pipeline simultaneously. Example: a single-port memory being accessed for both\n— 3 —\nJISCE / IT / R23 / IT301 / Sample Paper 2COA — Answer Key\ninstruction fetch and data read in the same cycle. Solution: resource duplication (e.g., separate\nI-cache and D-cache)."
    },
    "A8": {
      "question_number": "A8",
      "correct_option": "b",
      "solution": "Pagingdivides the logical address space into fixed-size blocks calledpagesand physical mem-\nory into equal-sizedframes. Segmentation uses variable-size blocks based on logical program\nstructure. Paging eliminates external fragmentation."
    },
    "A9": {
      "question_number": "A9",
      "correct_option": "c",
      "solution": "CISC instructions are complex andmay take multiple clock cyclesto execute. CISC typi-\ncally uses microprogrammed control (not hardwired), has a large instruction set with variable-\nlength instructions, and supports complex addressing modes. Options (a), (b), (d) describe\nRISC."
    },
    "A10": {
      "question_number": "A10",
      "correct_option": "b",
      "solution": "Theinclusion propertymeans each level of the hierarchy contains asubsetof the data in\nthe level below it. Registers⊂Cache⊂Main Memory⊂Disk. The upper (faster, smaller) level\nholds the most frequently accessed subset."
    },
    "A11": {
      "question_number": "A11",
      "solution": "— 4 —\nJISCE / IT / R23 / IT301 / Sample Paper 2COA — Answer Key\n(d) Database queries\nAnswer: (b)\nSystolic arraysare specialized architectures where data flows rhythmically through a network\nof processing elements (like blood through the heart — hence “systolic”). They are ideal for\nregular, repetitive computations likematrix multiplication, convolution, and DSP operations.\nEach PE performs a simple operation and passes results to neighbors."
    },
    "A12": {
      "question_number": "A12",
      "correct_option": "b",
      "solution": "Indaisy-chain(serial) priority, devices are connected in a chain. The interrupt acknowledge\nsignal propagates from the CPU outward. The deviceclosest to the CPUintercepts the\nacknowledge first, giving it the highest priority. Devices farther away get lower priority.\n— 5 —\nJISCE / IT / R23 / IT301 / Sample Paper 2COA — Answer Key"
    },
    "B1": {
      "question_number": "B1",
      "solution": "Role of Compiler/Assembler:\nThe compiler translates high-level language (C, Java) into assembly or machine code. The\nassembler converts assembly language mnemonics into binary machine instructions with proper\nopcodes, operand fields, and addressing modes. Together they bridge the gap between human-\nreadable programs and machine-executable instructions stored in memory.\nRole of Operating System:\nMemory management:Allocates memory for program instructions and data; manages\nvirtual memory and paging\nProcess management:Loads the compiled program into memory, sets the PC to the\nstarting address, manages CPU scheduling among multiple programs\nI/O management:Handles device drivers, interrupt service routines, and DMA configu-\nration\nResource protection:Ensures programs don’t interfere with each other’s memory space\nExecution support:Handles exceptions, system calls, and program termination\nIn the stored program model, the OS itself is a program stored in memory, managing the\nexecution of user programs through the same fetch-decode-execute mechanism."
    },
    "B2": {
      "question_number": "B2",
      "solution": "ParameterRISCCISC\nInstruction SetSmall set of simple, uniform in-\nstructions. Fixed-length encod-\ning (e.g., 32 bits).\nLarge set of complex instruc-\ntions with variable-length en-\ncoding.\nControl UnitHardwired control — fast, uses\ncombinational logic circuits.\nMicroprogrammed control —\nflexible, uses microcode stored\nin control memory.\nCPIMost instructions complete in1\nclock cycle(ideal for pipelin-\ning).\nInstructions take2–15+ cy-\nclesdepending on complexity.\nAddressing\nModes\nFew, simple modes (register,\nimmediate,displacement).\nLoad/store architecture.\nMany complex modes (indi-\nrect, indexed, auto-increment,\netc.). Memory-to-memory op-\nerations.\nRegistersLarge register file (32–128 reg-\nisters).\nFewer general-purpose regis-\nters.\nExamplesARM, MIPS, RISC-V, SPARCx86, VAX, Motorola 68000\nKey insight:RISC moves complexity to the compiler (software), while CISC keeps complexity\nin hardware. Modern x86 processors internally decode CISC instructions into RISC-like micro-\nops, combining benefits of both.\n— 6 —\nJISCE / IT / R23 / IT301 / Sample Paper 2COA — Answer Key"
    },
    "B3": {
      "question_number": "B3",
      "solution": "(a) SRAM vs DRAM:\nFeatureSRAMDRAM\nStorage element6 transistors (flip-flop)1 transistor + 1 capacitor\nSpeedFaster (ns access time)Slower (tens of ns)\nDensityLower (6T per bit)Higher (1T-1C per bit)\nCostMore expensive per bitCheaper per bit\nRefresh neededNo (static)Yes (periodic refresh)\nPowerLower standby powerHigher (refresh circuits)\nUsageCache memory (L1, L2, L3)Main memory (RAM sticks)\n(b) Why DRAM needs refreshing:\nDRAM stores each bit as charge on a tinycapacitor. Due to leakage current through the\ntransistor and capacitor dielectric, this charge graduallydissipates over time(typically within\na few milliseconds). Without refreshing, the stored data would be lost.\nRefreshing involvesreading each rowof the memory array andwriting it back, which\nrestores the capacitor charge to its full level. This must be done every2–64 msfor all rows.\nDuring refresh, the memory is briefly unavailable, adding overhead."
    },
    "B4": {
      "question_number": "B4",
      "solution": "1. Data Hazard— arises from instruction dependencies:\nExample:ADD R1, R2, R3;SUB R4, R1, R5\nThe SUB needs R1 which ADD hasn’t written back yet (RAW dependency).\n2. Control Hazard— caused by branch instructions:\nExample:BEQ R1, R2, TARGET;ADD R3, R4, R5\nThe pipeline has already fetched ADD, but if the branch is taken, ADD must be flushed.\n3. Structural Hazard— hardware resource conflict:\nExample: IF stage fetching an instruction while MEM stage accesses data memory, with a\nsingle-port memory— both need memory in the same cycle.\nOperand Forwarding (Bypassing):\nInstead of waiting for the result to be written to the register file (WB stage), the result is\nforwarded directlyfrom the ALU output (end of EX stage) to the ALU input of the dependent\ninstruction:\nADD R1,R2,R3\nSUB R4,R1,R5\nIFID\nforward\nEX\nThe ALU result of ADD is forwarded to the EX stage input of SUB, eliminating the 2-cycle\nstall that would otherwise be needed.\n— 7 —\nJISCE / IT / R23 / IT301 / Sample Paper 2COA — Answer Key"
    },
    "B5": {
      "question_number": "B5",
      "solution": "(a) Instruction-Level Parallelism (ILP):\nILP is the measure of how many instructions in a program can beexecuted simultaneously.\nIt exploits the fact that many instructions in a program are independent of each other and can\nbe overlapped or executed in parallel. ILP is limited by true data dependencies, control depen-\ndencies, and resource constraints. Techniques to increase ILP include pipelining, superscalar\nissue, loop unrolling, register renaming, and speculative execution.\n(b) Superscalar vs Super-Pipelined:\nFeatureSuperscalarSuper-Pipelined\nApproachIssuesmultiple instruc-\ntionsper clock cycle\nDivides pipeline stages into\nfiner sub-stages\nHardwareMultipleexecutionunits\n(ALUs, FPUs)\nDeeper pipeline with more\nstages\nClock speedNormal clock rateHigher effective clock rate\nCPICPI<1 (multiple issue)CPI≈1 but faster clock\nHazard impactNeeds complex dependency\nchecking\nBranchpenaltiesincrease\nwith depth\nExampleIntel Core, ARM Cortex-AMIPS R4000 (8-stage)\n— 8 —\nJISCE / IT / R23 / IT301 / Sample Paper 2COA — Answer Key"
    },
    "C1": {
      "question_number": "C1",
      "solution": "(a) ALU Design:\nThe ALU (Arithmetic Logic Unit) performs arithmetic (add, subtract, multiply, divide) and\nlogic (AND, OR, XOR, NOT) operations. A basic ALU stage for biticonsists of:\nAfull adderfor arithmetic operations\nLogic gatesfor Boolean operations\nAmultiplexerto select the output (arithmetic vs logic result)\nControl lines(S\n1\n,S\n0\n) to select the specific operation\n4-bit Arithmetic Unit for Add/Subtract:\nFor addition:A+Bwith carry-inC\n0\n= 0. Each bit uses a full adder:S\ni\n=A\ni\n⊕B\ni\n⊕C\ni\n.\nFor subtraction (A−B): Uses 2’s complement —A−B=A+\nB+ 1. The control signal:\nInverts all bits ofBthrough XOR gates (eachB\ni\nis XORed with a mode select lineM)\nSets carry-inC\n0\n= 1 (to add the ‘+1’ for 2’s complement)\nWhenM= 0: XOR passesBunchanged,C\n0\n= 0→Addition.\nWhenM= 1: XOR invertsB,C\n0\n= 1→Subtraction (A+\n̄\nB+ 1 =A−B).\n(b) Booth’s Algorithm:(+9)×(−6)in 5-bit\n+9 = 01001,−6 = 11010 (2’s complement).\nMultiplicandM= 01001,−M= 10111.\nMultiplierQ= 11010, Initial:A= 00000,Q\n−1\n= 0, Count = 5.\nStepAQQ\n−1\nnOperation\nInit000001101005—\n110111110100Q\n0\nQ\n−1\n= 00→No op...\n(Full step-by-step with ASR after each operation)\nContinue for 5 iterations...\nFinalA Q= 10-bit result =−54\n10\nVerification\n(+9)×(−6) =−54✓\n−54 in 10-bit 2’s complement: 1111001010\n(c) Overflow and Underflow:\nOverflowoccurs when the result of an arithmetic operation exceeds the maximum representable\nvalue. Inn-bit 2’s complement: range is [−2\nn−1\n,+2\nn−1\n−1]. Adding two large positive numbers\nmay produce a negative result (overflow).\nUnderflowoccurs when the result is smaller (more negative) than the minimum representable\nvalue. Adding two large negative numbers may produce a positive result.\nDetection in 2’s complement:\n— 9 —\nJISCE / IT / R23 / IT301 / Sample Paper 2COA — Answer Key\nOverflow is detected whenC\nn−1\n̸=C\nn\n(carry into MSB̸= carry out of MSB):\nOverflow =C\nn−1\n⊕C\nn\nEquivalently: overflow occurs when adding two numbers of thesame signproduces a result of\ntheopposite sign.\n+5 + (+4) = +9exceeds 4-bit range [−8,+7]→Overflow\n−6 + (−5) =−11exceeds 4-bit range [−8,+7]→Underflow\n+3 + (−2) = +1within range→No overflow"
    },
    "C2": {
      "question_number": "C2",
      "solution": "(a) Memory Hierarchy:\nRegisters\nCache (SRAM)\nMain Memory (DRAM)\nSecondary Storage (Disk/SSD)\nFastest, smallest, most expensive\nSlowest, largest, cheapest\nSpeed decreases / Size increases\nTrade-offs:Faster memory is smaller and more expensive per bit. Registers (ps access)<1\nKB; Cache (ns)∼KB–MB; DRAM (tens of ns)∼GB; Disk (ms)∼TB. The hierarchy exploits\nlocality to give the illusion of large, fast memory at low cost.\n(b) Cache Mapping Calculations:\nMain memory = 4 MB = 2\n22\nbytes. Cache = 256 KB = 2\n18\nbytes. Block = 64 = 2\n6\nbytes.\nPhysical address = 22 bits. Number of cache lines = 2\n18\n/2\n6\n= 2\n12\n= 4096. Offset bits = 6.\n(i) Direct Mapped:Line bits = log\n2\n(4096) = 12, Tag = 22−12−6 =4bits.\n(ii) Fully Associative:No index field. Tag = 22−6 =16bits.\n(iii) 8-way Set Associative:Sets = 4096/8 = 512 = 2\n9\n. Set bits = 9. Tag = 22−9−6 =7\nbits.\nMappingTagIndex/SetOffset\nDirect Mapped4 bits12 bits6 bits\nFully Associative16 bits—6 bits\n8-way Set Assoc.7 bits9 bits6 bits\n(c) Coherence and Locality:\nCoherence:In a multiprocessor system with multiple caches, coherence ensures that all pro-\ncessors see a consistent view of memory. If one processor modifies a cache line, all other caches\nwith copies must be updated or invalidated (protocols: MESI, MOESI).\nLocality:\n— 10 —\nJISCE / IT / R23 / IT301 / Sample Paper 2COA — Answer Key\nTemporal locality:Recently accessed data is likely to be accessed again soon. Cache\nretains recently used blocks; LRU replacement exploits this.\nSpatial locality:Data near recently accessed locations is likely to be accessed soon. Caches\nfetch entire blocks (not single bytes), bringing nearby data into cache automatically.\nCache design exploits both: block size optimizes spatial locality; replacement policies optimize\ntemporal locality."
    },
    "C3": {
      "question_number": "C3",
      "solution": "(a) Hardwired Control Unit Design:\nA hardwired control unit usescombinational and sequential logic circuitsto generate\ncontrol signals.\nKey components:\nInstruction Decoder:Takes the opcode from the IR and activates one of 2\nn\noutput lines\n(wheren= opcode bits). Each output corresponds to a specific instruction type.\nSequence Counter (SC):A counter (typically 4-bit) that tracks the current step within an\ninstruction’s execution. It is cleared at the start of each new instruction and incremented after\neach micro-operation. SC determines the timing of control signals.\nControl Logic Gates:AND/OR gates that combine decoder outputs, SC values, and condition\nflags to generate the correct control signals at each step. For example:\nSignal\nREAD\n=D\n0\n·T\n2\n+D\n1\n·T\n3\n+...\nwhereD\ni\n= decoder output,T\nj\n= timing signal from SC.\n(b) Hardwired vs Microprogrammed:\nFeatureHardwiredMicroprogrammed\nSpeedFasterSlower (control memory ac-\ncess)\nFlexibilityHard to modifyEasytomodify(change\nROM)\nComplexityDifficult for large ISAHandles complexity well\nCostLess (no control memory)More (needs control memory)\nUsageRISC processorsCISC processors\nHorizontal microinstruction:Long word where each bit directly controls one control signal.\nFast (one level of decoding) but wasteful (many bits, mostly 0s). Width = number of control\nsignals.\nVertical microinstruction:Shorter word with encoded fields that are decoded to generate\ncontrol signals. More compact but slower (needs additional decoding logic). Width≪number\nof control signals.\n(c) Polled I/O vs Interrupt-Driven I/O:\nPolled I/O (Programmed I/O):The CPU repeatedly checks (polls) the status register of\nI/O devices in a loop to see if data is ready. The CPU isbusy-waitingand cannot do other\nwork. Simple but wastes CPU cycles.\nInterrupt-Driven I/O:The CPU issues an I/O command and continues other work. When\nthe device is ready, it sends aninterrupt signal. The CPU suspends current execution, runs\ntheInterrupt Service Routine (ISR)to handle the transfer, then resumes its previous task.\n— 11 —\nJISCE / IT / R23 / IT301 / Sample Paper 2COA — Answer Key\nWhy interrupt-driven is preferred:\nCPU is free to execute other instructions (no busy-waiting)\nBetter CPU utilization and system throughput\nSuitable for asynchronous, unpredictable I/O events\nOnly overhead is the context switch when interrupt occurs"
    },
    "C4": {
      "question_number": "C4",
      "solution": "(a) Instruction Pipeline vs Arithmetic Pipeline:\nInstruction pipeline:Overlaps different phases (IF, ID, EX, MEM, WB) of successive in-\nstructions. Each stage handles a different instruction simultaneously.\nArithmetic pipeline:Decomposes a complex arithmetic operation (e.g., floating-point addi-\ntion) into sub-operations executed in pipeline stages. Example: FP addition stages — exponent\ncomparison, mantissa alignment, mantissa addition, normalization.\nSpace-Time Diagram (4-stage, 6 instructions):\nT\n1\nT\n2\nT\n3\nT\n4\nT\n5\nT\n6\nT\n7\nT\n8\nT\n9\nS\n1\nI\n1\nI\n2\nI\n3\nI\n4\nI\n5\nI\n6\nS\n2\nI\n1\nI\n2\nI\n3\nI\n4\nI\n5\nI\n6\nS\n3\nI\n1\nI\n2\nI\n3\nI\n4\nI\n5\nI\n6\nS\n4\nI\n1\nI\n2\nI\n3\nI\n4\nI\n5\nI\n6\nTotal time = (k+n−1) = (4 + 6−1) = 9 clock cycles.\n(b) Pipeline Performance Calculation:\nStage delays: 120, 100, 150, 130, 110 ns. Register delay: 5 ns.\n(i) Clock cycle time= max(stage delays) + register delay\nτ= 150 + 5 =155ns\n(ii) Time for 500 tasks:\nT\npipe\n= (k+n−1)×τ= (5 + 500−1)×155 = 504×155 =78,120ns\n(iii) Non-pipelined time per task= 120 + 100 + 150 + 130 + 110 = 610 ns\nT\nnon\n= 500×610 = 305,000 ns\nS=\n305,000\n78,120\n≈3.905\nMaximum speedup asn→ ∞:S\nmax\n= 610/155≈3.94 (less thank= 5 due to pipeline\nimbalance).\n(c) Pipeline Optimization Techniques:\nDelayed Branching:The instruction in the delay slot (immediately after the branch) is\nalways executed, regardless of branch outcome. The compiler fills this slot with a useful\ninstruction that is independent of the branch. Eliminates one cycle of branch penalty.\nBranch Prediction:Guesses the branch outcome before it is resolved.\n— 12 —\nJISCE / IT / R23 / IT301 / Sample Paper 2COA — Answer Key\nStatic:Always predict taken/not-taken, or predict based on branch direction\nDynamic:Uses Branch History Table (BHT) with 1-bit or 2-bit saturating counters; Branch\nTarget Buffer (BTB) caches target addresses\nInstruction Reordering:The compiler rearranges instruction order to fill pipeline bubbles\ncaused by data dependencies, without changing program semantics. Independent instructions\nare moved into slots that would otherwise be stalls. Also known as instruction scheduling.\nQ11: Short notes — CAM, Virtual memory, Non-Von Neumann, Flynn’s, Clusters\n(15) CO3 BL3\nWrite short notes on anythreeof the following:\n(a) Associative memory (Content Addressable Memory)(b) Virtual memory organization\nand page replacement policies(c) Non-Von Neumann architectures: Dataflow and Reduction\ncomputers(d) Taxonomy of parallel architectures (Flynn’s classification with examples)(e)\nCluster computers and their advantages\nSolution — Any Three\n(a) Associative Memory (CAM): [5 marks]\nAssociative memory (Content Addressable Memory) is accessed bycontent rather than ad-\ndress. A search word (key) is compared simultaneously with all stored words using parallel\ncomparison logic. It returns the data associated with the matching entry.\nStructure:Each cell has storage + comparison logic. An argument register holds the search\nkey; a key register acts as a mask to select which bits to compare. Match lines indicate which\nwords match.\nApplications:TLB (page table cache), cache tag comparison in fully-associative caches, rout-\ning tables in network switches.\nAdvantages:Very fast search (O(1)).Disadvantages:Expensive hardware (comparator per\nbit per word), high power consumption, limited capacity.\n(b) Virtual Memory & Page Replacement: [5 marks]\nVirtual memory uses disk as an extension of main memory, allowing programs larger than\nphysical memory to execute. The OS divides logical memory intopagesand physical memory\nintoframes.\nPage Table:Maps virtual page numbers to physical frame numbers with valid, dirty, and\nreference bits. Multi-level page tables reduce memory overhead.\nPage Replacement Policies:\nFIFO:Replace oldest page.Simple but may replace frequently-used pages (Belady’s\nanomaly).\nLRU:Replace least recently used page. Good performance; implemented with counters or\nstack.\nOptimal:Replace page not needed for longest future time. Theoretical lower bound (not\npractically implementable).\nClock (Second Chance):Circular list with reference bits; approximates LRU efficiently.\n(c) Non-Von Neumann Architectures: [5 marks]\nDataflow Computers:Instructions are executed when all their operands become available\n(data-driven execution), rather than following a sequential program counter. Programs are\nrepresented asdataflow graphswith nodes (operations) and arcs (data paths). Tokens carry\ndata along arcs; a node fires when all input tokens arrive. Naturally parallel; no Von Neumann\nbottleneck. Challenges: token management, overhead in matching tokens.\nReduction Computers:Follow demand-driven (lazy) evaluation — computations are per-\nformed only when their results areneeded. Based on functional programming models. An ex-\npression is reduced by repeatedly replacing reducible sub-expressions with their values. Graph\n— 13 —\nJISCE / IT / R23 / IT301 / Sample Paper 2COA — Answer Key\nreduction is a common implementation technique. Avoids unnecessary computations but has\noverhead in managing the reduction graph.\n(d) Flynn’s Taxonomy: [5 marks]\nTypeDescriptionExamples\nSISDSingle instruction, sin-\ngle data stream\nTraditionaluniprocessor(Intel\n8086)\nSIMDSingleinstruction,\nmultiple data streams\nVector processors (Cray), GPU\nshaders, array processors\nMISDMultiple instructions,\nsingle data stream\nRarely implemented; systolic arrays\n(debated)\nMIMDMultiple instructions,\nmultiple data streams\nMulticore CPUs,clusters,dis-\ntributed systems\n(e) Cluster Computers: [5 marks]\nA cluster is a group ofinterconnected commodity computers(nodes) that work together\nas a unified computing resource, coordinated by middleware software.\nCharacteristics:Each node has its own processor, memory, and OS. Nodes are connected\nvia a high-speed network (Ethernet, InfiniBand). Message passing (MPI) is used for inter-node\ncommunication.\nAdvantages:\nCost-effective:Uses commodity hardware instead of expensive supercomputers\nScalable:Add more nodes to increase performance\nHigh availability:If one node fails, others continue (fault tolerance)\nFlexible:Nodes can be heterogeneous or upgraded independently\nTypes:High-Availability (HA) clusters, Load-Balancing clusters, High-Performance Comput-\ning (HPC) clusters (e.g., Beowulf clusters).\n— 14 —"
    }
  }
}